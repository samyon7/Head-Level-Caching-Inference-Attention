{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class HeadInferTransformerLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, device):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "        self.device = device\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.W_q.to(device)\n",
        "        self.W_k.to(device)\n",
        "        self.W_v.to(device)\n",
        "        self.W_o.to(device)\n",
        "\n",
        "        self.num_heads_on_gpu = num_heads\n",
        "\n",
        "    def forward(self, x, kv_cache):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "\n",
        "        Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        all_head_outputs = []\n",
        "        for h in range(self.num_heads):\n",
        "            if h < self.num_heads_on_gpu:\n",
        "\n",
        "                K = self.W_k(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)[:, h:h+1, :, :]\n",
        "                V = self.W_v(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)[:, h:h+1, :, :]\n",
        "                K_cache = kv_cache['K'][h].to(self.device)\n",
        "                V_cache = kv_cache['V'][h].to(self.device)\n",
        "                K = torch.cat([K_cache, K], dim=2)\n",
        "                V = torch.cat([V_cache, V], dim=2)\n",
        "                kv_cache['K'][h] = K.cpu()\n",
        "                kv_cache['V'][h] = V.cpu()\n",
        "                attention_scores = torch.matmul(Q[:, h:h+1, :, :], K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))\n",
        "                attention_probs = self.softmax(attention_scores)\n",
        "                head_output = torch.matmul(attention_probs, V)\n",
        "\n",
        "            else:\n",
        "                K_cache = kv_cache['K'][h].to(self.device)\n",
        "                V_cache = kv_cache['V'][h].to(self.device)\n",
        "                K = self.W_k(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)[:, h:h+1, :, :].cpu()\n",
        "                V = self.W_v(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)[:, h:h+1, :, :].cpu()\n",
        "                K = torch.cat([K_cache, K], dim=2)\n",
        "                V = torch.cat([V_cache, V], dim=2)\n",
        "                kv_cache['K'][h] = K.cpu()\n",
        "                kv_cache['V'][h] = V.cpu()\n",
        "                K = K.to(self.device)\n",
        "                V = V.to(self.device)\n",
        "                attention_scores = torch.matmul(Q[:, h:h+1, :, :], K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))\n",
        "                attention_probs = self.softmax(attention_scores)\n",
        "                head_output = torch.matmul(attention_probs, V)\n",
        "            all_head_outputs.append(head_output)\n",
        "        all_head_outputs = torch.cat(all_head_outputs, dim=1)\n",
        "        output = self.W_o(all_head_outputs.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model))\n",
        "        return output\n",
        "\n",
        "def initialize_kv_cache(batch_size, num_heads, seq_len, head_dim, device='cpu'):\n",
        "  kv_cache = {'K': [], 'V': []}\n",
        "  for _ in range(num_heads):\n",
        "    kv_cache['K'].append(torch.zeros(batch_size, 1, 0, head_dim, device=device))\n",
        "    kv_cache['V'].append(torch.zeros(batch_size, 1, 0, head_dim, device=device))\n",
        "  return kv_cache\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    batch_size = 2\n",
        "    seq_len = 32\n",
        "    d_model = 256\n",
        "    num_heads = 8\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    layer = HeadInferTransformerLayer(d_model, num_heads, device)\n",
        "    layer.to(device)\n",
        "    x = torch.randn(batch_size, seq_len, d_model).to(device)\n",
        "\n",
        "    kv_cache = initialize_kv_cache(batch_size, num_heads, seq_len, d_model // num_heads)\n",
        "    output = layer(x, kv_cache)\n",
        "\n",
        "    print(\"Output shape:\", output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_dHUadlqyS9",
        "outputId": "fbcbdaab-a61b-4f9b-f8a5-c901e9dad702"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([2, 32, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6Loe0QGtrGUF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}